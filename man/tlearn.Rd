% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tlearn-package.R, R/tlearn.R
\docType{package}
\name{tlearn}
\alias{tlearn}
\alias{_PACKAGE}
\alias{tlearn-package}
\title{tlearn: Transfer Learn}
\usage{
tlearn(
  A,
  w,
  h.init = NULL,
  L0 = ncol(w),
  L1 = 0,
  L2 = 0,
  PE = 0,
  weights = NULL,
  min = 0,
  max = 1e+10,
  values = NULL,
  parallel = TRUE,
  maxit = 50,
  tol = 1e-08
)
}
\arguments{
\item{A}{samples to project: matrix of features x samples, dense or sparse}

\item{w}{feature coefficients model: matrix of features x factors}

\item{h.init}{initial sample coefficients model: optional matrix of factors x samples (leave NULL for random initialization)}

\item{L0}{L0 truncation on each sample in "h", the number of non-zero values in each column. Must be less than the number of columns in "w" (rank of the model)}

\item{L1}{L1/Lasso regularization to be applied on "h"}

\item{L2}{L2/Ridge regression to be applied on "h"}

\item{PE}{pattern extraction penalty to be applied on "h" (i.e. force patterns of weightings to be different across rows of "h")}

\item{weights}{optional weights/maskings given as a vector of ncol(A), nrow(A), or a dense or sparse matrix of dim(A). Leave NULL for no weighting}

\item{min}{minimum permitted value, zero for NMF}

\item{max}{maximum permitted value, default 1e10}

\item{values}{optional, constrain projection values to a multinomial distribution containing these values}

\item{parallel}{logical, use all available threads}

\item{maxit}{maximum number of iterations for least squares sequential coordinate descent}

\item{tol}{tolerance for convergence of the least squares sequential coordinate descent solver}
}
\value{
h, a sample embeddings matrix of dimensions "factors x samples".
}
\description{
Fast and flexible sample projection onto feature models for transfer learning with constraints, regularization, and weights

Project samples (A) onto a factor model (w) to find the best sample embedding (h)
}
\details{
Fast and flexible transfer learning.

The factor model may be any dimensional reduction (commonly NMF, PCA, or SVD).

If "A" and "w" contain non-intersecting features, only features which intersect will be retained.

If you'd like to monitor the progress of a very large projection and do not need weights, regularizations, or a warm start initialization, see \code{\link{tlearn_minibatch}}
}
\section{Why tlearn?}{

\itemize{
\item fast parallelized constrained least squares coordinate descent solver
\item sparse or dense matrix support in dedicated backends
\item regularization (L1/Lasso, L2/Ridge, PE/Pattern Extraction) and truncation (L0 on the model or individual samples)
\item constraints to a single range, multiple ranges, or a multinomial distribution
\item weighting/masking
\item minibatching
\item utilities for post-projection analysis
}
}

\section{What is transfer learning?}{

Given the right hand side of any matrix decomposition or feature model (matrix of features x factors), tlearn finds how represented each factor is in each sample.
For example, a model describing facial features can be projected onto a set of face portraits to learn what features are present in that face. The resulting model will give a coefficient for each feature, and do so for each portrait.
}

\section{There's a vignette!}{

The vignette succintly covers theoretical concepts, best practices, and demonstrates transfer learning using PCA, NMF, and UMAP as examples to explore single cell type inference and spatiotemporal patterns of bird species frequency.
}

\section{Functions in the tlearn package}{

\itemize{
\item \strong{tlearn}: project samples onto factor models
\item \strong{tlearnMinibatch}: minibatch tlearn for large projections
\item \strong{gnnls}: generalized constrained non-negative least squares, least squares solver behind tlearn
\item \strong{loss}: error of a projection model
\item \strong{rank}: rank features in a projection by total weight and scale "h" by the rank (diagonal) values
\item \strong{sampleLoss}: error of a projection model for each sample. Find samples that are poorly or well explained by the factor model.
\item \strong{align}: align factors in two models to facilitate direct comparisons
\item \strong{purity}: cosine distance between samples and factors as a measure of factor expression purity in each sample
}
}

\seealso{
Useful links:
\itemize{
  \item \url{https://github.com/zdebruine/tlearn}
  \item Report bugs at \url{https://github.com/zdebruine/tlearn/issues}
}


\code{\link{tlearn_minibatch}}
}
